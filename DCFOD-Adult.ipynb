{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oJMhoA9qZxuR"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import logging\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from torch.autograd import Function\n",
    "\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "typPJi-4bA_s"
   },
   "outputs": [],
   "source": [
    "class DCFOD(nn.Module):\n",
    "    \"\"\"\n",
    "    DCFOD consists of a encoder, decoder, discriminator, and cluster centroid layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, num_classes, num_features, num_attributes, cuda):\n",
    "        super(DCFOD, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = num_features\n",
    "        self.num_attributes = num_attributes\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(input_size, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 2000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2000, num_features)\n",
    "        ).to(cuda)\n",
    "\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Linear(num_features, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 2000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2000, num_attributes)\n",
    "        ).to(cuda)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(num_features, 2000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2000, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, input_size)\n",
    "        ).to(cuda)\n",
    "\n",
    "        self.clusterCenter = nn.Parameter(torch.zeros(num_classes, num_features).to(cuda))\n",
    "\n",
    "        self.alpha = 1.0\n",
    "        self.clusteringMode = False\n",
    "        self.validateMode = False\n",
    "\n",
    "        # -----model initialization----- #\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight).to(cuda)\n",
    "\n",
    "    def setClusteringMode(self, mode):\n",
    "        self.clusteringMode = mode\n",
    "\n",
    "    def setValidateMode(self, mode):\n",
    "        self.validateMode = mode\n",
    "\n",
    "    def getDistanceToClusters(self, x):\n",
    "        \"\"\"\n",
    "        obtain the distance to cluster centroids for each instance\n",
    "        Args:\n",
    "            x: sample on the embedded space\n",
    "        Returns: square of the euclidean distance, and the euclidean distance\n",
    "        \"\"\"\n",
    "        xe = torch.unsqueeze(x, 1) - self.clusterCenter\n",
    "        dist_to_centers = torch.sum(torch.mul(xe, xe), 2)\n",
    "        euclidean_dist = torch.sqrt(dist_to_centers)\n",
    "\n",
    "        return dist_to_centers, euclidean_dist\n",
    "\n",
    "    def forward(self, x):\n",
    "        # -----feature embedding----- #\n",
    "        x = x.view(-1, self.input_size)\n",
    "        x_e = self.encoder(x)\n",
    "\n",
    "        # -----if only wants to initialize cluster centroids\n",
    "        # or validate the performance on the whole dataset, return x_e----- #\n",
    "        if self.clusteringMode or self.validateMode:\n",
    "            return x_e\n",
    "\n",
    "        # -----else the discriminator predicts the subgroup assignment for each instance----- #\n",
    "        reversed_x_e = GradientReversalLayer.apply(x_e)\n",
    "        x_sa = self.discriminator(reversed_x_e)\n",
    "\n",
    "        # -----if in training, return the embedded x, decoded x, and subgroup discrimination----- #\n",
    "        x_de = self.decoder(x_e)\n",
    "        x_de = x_de.view(-1, 1, self.input_size)\n",
    "\n",
    "        return x_e, x_de, x_sa\n",
    "\n",
    "\n",
    "class GradientReversalLayer(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output.neg(), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bq-25fIeZ3AJ"
   },
   "outputs": [],
   "source": [
    "gpu = '1'\n",
    "if gpu == '0':\n",
    "    cuda = torch.device('cuda:0')\n",
    "    devices = [0, 1, 2, 3]\n",
    "elif gpu == '1':\n",
    "    cuda = torch.device('cuda:1')\n",
    "    devices = [1, 2, 3, 0]\n",
    "elif gpu == '2':\n",
    "    cuda = torch.device('cuda:2')\n",
    "    devices = [2, 3, 0, 1]\n",
    "elif gpu == '3':\n",
    "    cuda = torch.device('cuda:3')\n",
    "    devices = [3, 0, 1, 2]\n",
    "else:\n",
    "    raise NameError('no more GPUs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7abHJd-pZ5b6"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    print(f\"setting seed to {seed}\")\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # torch.use_deterministic_algorithms(True)\n",
    "    # torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_sv_2_jsaEwH"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import classification_report, confusion_matrix, average_precision_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def set_seed(seed = 0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def get_samples(dataset, upper=None, lower=None):\n",
    "    df_X = pd.DataFrame(dataset.X.reshape(-1, 20))\n",
    "    df_X['y'] = dataset.y\n",
    "    df_X_cf = pd.DataFrame(dataset.X_cf.reshape(-1, 20))\n",
    "    df_X_cf['y'] = dataset.y_cf\n",
    "    # lst_y = df_X['y'].values\n",
    "    lst_y = np.array(np.concatenate((df_X['y'].values, df_X_cf['y'].values)))\n",
    "    if (upper is None) or (lower is None):\n",
    "        ab_upper = np.quantile(lst_y, 0.99)\n",
    "        ab_cf_uupper = np.quantile(lst_y, 0.90)\n",
    "        ab_cf_ulower = np.quantile(lst_y, 0.85)\n",
    "        # ab_cf_lupper = np.quantile(lst_y, 0.05)\n",
    "        # ab_cf_llower = np.quantile(lst_y, 0.04)\n",
    "        n_cf_upper = np.quantile(lst_y, 0.7)\n",
    "        n_cf_lower = np.quantile(lst_y, 0.68)\n",
    "        ab_lower = np.quantile(lst_y, 0.01)\n",
    "        n_lower = np.quantile(lst_y, 0.3)\n",
    "        n_upper = np.quantile(lst_y, 0.4)\n",
    "    lst_n_c, lst_n_nc, lst_n_m, lst_ab_c, lst_ab_nc, lst_ab_m = [], [], [], [], [], []\n",
    "    lst_n_c_cf, lst_n_nc_cf, lst_n_m_cf, lst_ab_c_cf, lst_ab_nc_cf, lst_ab_m_cf = [], [], [], [], [], []\n",
    "\n",
    "    df_n = df_X.loc[(df_X['y'] > n_lower) & (df_X['y'] < n_upper)]\n",
    "    df_n_cf = df_X_cf.loc[df_n.index]\n",
    "\n",
    "    # df_n_c_cf = df_n_cf.loc[((df_n_cf['y'] > ab_cf_ulower) & (df_n_cf['y'] < ab_cf_uupper)) | ((df_n_cf['y'] > ab_cf_llower) & (df_n_cf['y'] < ab_cf_lupper))]\n",
    "    df_n_c_cf = df_n_cf.loc[(df_n_cf['y'] > ab_cf_ulower) & (df_n_cf['y'] < ab_cf_uupper)]\n",
    "    df_n_c = df_n.loc[df_n_c_cf.index]\n",
    "    lst_n_c.extend(df_n_c.values)\n",
    "    lst_n_c_cf.extend(df_n_c_cf.values)\n",
    "\n",
    "    df_n_nc_cf = df_n_cf.loc[(df_n_cf['y'] > n_lower) & (df_n_cf['y'] < n_upper)]\n",
    "    df_n_nc = df_n.loc[df_n_nc_cf.index]\n",
    "    lst_n_nc.extend(df_n_nc.values)\n",
    "    lst_n_nc_cf.extend(df_n_nc_cf.values)\n",
    "\n",
    "    df_ab = df_X.loc[(df_X['y'] < ab_lower) | (df_X['y'] > ab_upper)]\n",
    "    df_ab_cf = df_X_cf.loc[df_ab.index]\n",
    "\n",
    "    df_ab_nc_cf = df_ab_cf.loc[(df_ab_cf['y'] < ab_lower) | (df_ab_cf['y'] > ab_upper)]\n",
    "    df_ab_nc = df_ab.loc[df_ab_nc_cf.index]\n",
    "    lst_ab_nc.extend(df_ab_nc.values)\n",
    "    lst_ab_nc_cf.extend(df_ab_nc_cf.values)\n",
    "\n",
    "    df_ab_c_cf = df_ab_cf.loc[(df_ab_cf['y'] > n_cf_lower) & (df_ab_cf['y'] < n_cf_upper)]\n",
    "    df_ab_c = df_ab.loc[df_ab_c_cf.index]\n",
    "    lst_ab_c.extend(df_ab_c.values)\n",
    "    lst_ab_c_cf.extend(df_ab_c_cf.values)\n",
    "\n",
    "    lst_temp = lst_n_c.copy()\n",
    "    lst_temp.extend(lst_n_nc)\n",
    "\n",
    "    #     lst_temp.extend(lst_n_m)\n",
    "    lst_changed = [1 for _ in range(len(lst_n_c))]\n",
    "    lst_changed.extend([0 for _ in range(len(lst_n_nc))])\n",
    "    #     lst_changed.extend([-1 for _ in range(len(lst_n_m))])\n",
    "    df_n = pd.DataFrame(lst_temp)\n",
    "    df_n['changed'] = lst_changed\n",
    "    df_n['label'] = 0\n",
    "    df_n = df_n.sample(n=len(df_n), random_state=42)\n",
    "\n",
    "    lst_temp = lst_ab_c.copy()\n",
    "    lst_temp.extend(lst_ab_nc)\n",
    "    #     lst_temp.extend(lst_ab_m)\n",
    "    lst_changed = [1 for _ in range(len(lst_ab_c))]\n",
    "    lst_changed.extend([0 for _ in range(len(lst_ab_nc))])\n",
    "    #     lst_changed.extend([-1 for _ in range(len(lst_ab_m))])\n",
    "    df_ab = pd.DataFrame(lst_temp)\n",
    "    df_ab['changed'] = lst_changed\n",
    "    df_ab['label'] = 1\n",
    "\n",
    "    lst_temp = lst_n_c_cf.copy()\n",
    "    lst_temp.extend(lst_n_nc_cf)\n",
    "    #     lst_temp.extend(lst_n_m_cf)\n",
    "    lst_changed = [1 for _ in range(len(lst_n_c_cf))]\n",
    "    lst_changed.extend([0 for _ in range(len(lst_n_nc_cf))])\n",
    "    #     lst_changed.extend([-1 for _ in range(len(lst_n_m_cf))])\n",
    "    lst_label = [1 for _ in range(len(lst_n_c_cf))]\n",
    "    lst_label.extend([0 for _ in range(len(lst_n_nc_cf))])\n",
    "    #     lst_label.extend([0 for _ in range(len(lst_n_m_cf))])\n",
    "    df_n_cf = pd.DataFrame(lst_temp)\n",
    "    df_n_cf['changed'] = lst_changed\n",
    "    df_n_cf['label'] = lst_label\n",
    "    df_n_cf = df_n_cf.loc[df_n.index]\n",
    "\n",
    "    lst_temp = lst_ab_c_cf.copy()\n",
    "    lst_temp.extend(lst_ab_nc_cf)\n",
    "    #     lst_temp.extend(lst_ab_m_cf)\n",
    "    lst_changed = [1 for _ in range(len(lst_ab_c_cf))]\n",
    "    lst_changed.extend([0 for _ in range(len(lst_ab_nc_cf))])\n",
    "    #     lst_changed.extend([-1 for _ in range(len(lst_ab_m_cf))])\n",
    "    lst_label = [0 for _ in range(len(lst_ab_c_cf))]\n",
    "    lst_label.extend([1 for _ in range(len(lst_ab_nc_cf))])\n",
    "    #     lst_label.extend([0 for _ in range(len(lst_ab_m_cf))])\n",
    "    df_ab_cf = pd.DataFrame(lst_temp)\n",
    "    df_ab_cf['changed'] = lst_changed\n",
    "    df_ab_cf['label'] = lst_label\n",
    "\n",
    "    major = len(df_n.loc[df_n[0] == 1])\n",
    "    minor = int(major * 0.1)\n",
    "    df_n_nc_major = df_n.loc[(df_n[0] == 1) & (df_n['changed'] == 0)]\n",
    "    df_n_c_major = df_n.loc[(df_n[0] == 1) & (df_n['changed'] == 1)]\n",
    "    #     df_n_m_major = df_n.loc[(df_n[0]==1) & (df_n['changed']==-1)]\n",
    "    df_n_nc_minor = df_n.loc[(df_n[0] == -1) & (df_n['changed'] == 0)]\n",
    "    df_n_c_minor = df_n.loc[(df_n[0] == -1) & (df_n['changed'] == 1)]\n",
    "    #     df_n_m_minor = df_n.loc[(df_n[0]==-1) & (df_n['changed']==-1)]\n",
    "    print(f\"Normal major nc:{len(df_n_nc_major)}, c:{len(df_n_c_major)}\")\n",
    "    print(f\"Normal minor nc:{len(df_n_nc_minor)}, c:{len(df_n_c_minor)}\")\n",
    "\n",
    "    df_ab_nc_major = df_ab.loc[(df_ab[0] == 1) & (df_ab['changed'] == 0)]\n",
    "    df_ab_c_major = df_ab.loc[(df_ab[0] == 1) & (df_ab['changed'] == 1)]\n",
    "    #     df_ab_m_major = df_ab.loc[(df_ab[0]==1) & (df_ab['changed']==-1)]\n",
    "    df_ab_nc_minor = df_ab.loc[(df_ab[0] == -1) & (df_ab['changed'] == 0)]\n",
    "    df_ab_c_minor = df_ab.loc[(df_ab[0] == -1) & (df_ab['changed'] == 1)]\n",
    "    #     df_ab_m_minor = df_ab.loc[(df_ab[0]==-1) & (df_ab['changed']==-1)]\n",
    "    print(f\"Abnormal major nc:{len(df_ab_nc_major)}, c:{len(df_ab_c_major)}\")\n",
    "    print(f\"Abnormal minor nc:{len(df_ab_nc_minor)}, c:{len(df_ab_c_minor)}\")\n",
    "\n",
    "    n_train = 3000\n",
    "    df_train = pd.concat([df_n_nc_major.iloc[:n_train], df_n_c_major.iloc[:n_train], \\\n",
    "                          df_n_nc_minor.iloc[:n_train], df_n_c_minor.iloc[:n_train]])\n",
    "    df_train = df_train.sample(n=len(df_train), random_state=42)\n",
    "    df_eval_set = pd.concat([df_n_nc_major.iloc[0:1], df_n_c_major.iloc[0:1], \\\n",
    "                             df_n_nc_minor.iloc[0:1], df_n_c_minor.iloc[0:1]])\n",
    "    df_test_n = pd.concat([df_n_nc_major.iloc[n_train:n_train+1000], df_n_c_major.iloc[n_train:n_train+1000], \\\n",
    "                           df_n_nc_minor.iloc[n_train:n_train+1000], df_n_c_minor.iloc[n_train:n_train+1000]])\n",
    "    df_test_ab = pd.concat([df_ab_nc_major.iloc[:100], df_ab_c_major.iloc[:100], \\\n",
    "                            df_ab_nc_minor.iloc[:100], df_ab_c_minor.iloc[:100]])\n",
    "\n",
    "    df_train_cf = df_n_cf.loc[df_train.index]\n",
    "    df_eval_set_cf = df_n_cf.loc[df_eval_set.index]\n",
    "    df_test_n_cf = df_n_cf.loc[df_test_n.index]\n",
    "    df_test_ab_cf = df_ab_cf.loc[df_test_ab.index]\n",
    "\n",
    "    df_eval = df_train.iloc[-1000:].reset_index(drop=True)\n",
    "    df_train = df_train.iloc[:-1000].reset_index(drop=True)\n",
    "    df_eval_set.reset_index(drop=True, inplace=True)\n",
    "    df_test = pd.concat([df_test_n, df_test_ab]).reset_index(drop=True)\n",
    "\n",
    "    df_eval_cf = df_train_cf.iloc[-1000:].reset_index(drop=True)\n",
    "    df_train_cf = df_train_cf.iloc[:-1000].reset_index(drop=True)\n",
    "    df_eval_set_cf.reset_index(drop=True, inplace=True)\n",
    "    df_test_cf = pd.concat([df_test_n_cf, df_test_ab_cf]).reset_index(drop=True)\n",
    "    try:\n",
    "        df_train.to_csv('../data/train.csv')\n",
    "        df_test.to_csv('../data/test.csv')\n",
    "        df_test_cf.to_csv('../data/test_cf.csv')\n",
    "    except:\n",
    "        df_train.to_csv('data/train.csv')\n",
    "        df_test.to_csv('data/test.csv')\n",
    "        df_test_cf.to_csv('data/test_cf.csv')\n",
    "    return df_train, df_eval, df_test, df_eval_set, df_train_cf, df_eval_cf, df_test_cf, df_eval_set_cf\n",
    "\n",
    "\n",
    "class CFDataset(Dataset):\n",
    "    def __init__(self, X, do):\n",
    "        self.X = X\n",
    "        self.do = do\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return (self.X[idx], self.do[idx])\n",
    "\n",
    "\n",
    "def pretrain_split(df_train, df_eval, scaler, adult=0):\n",
    "    if not adult:\n",
    "        train_X = scaler.fit_transform(df_train.iloc[:, 1:-3].values.astype(np.float32))\n",
    "        lst_temp = [1 for _ in range(len(train_X))]\n",
    "\n",
    "        train_iter = DataLoader(CFDataset(torch.tensor(train_X.astype(np.float32)), torch.LongTensor(lst_temp)),\n",
    "                                batch_size=128, shuffle=True, worker_init_fn=np.random.seed(0))\n",
    "        # print(len(train_X))\n",
    "        eval_X = scaler.transform(df_eval.iloc[:, 1:-3].values.astype(np.float32))\n",
    "        lst_temp = [1 for _ in range(len(eval_X))]\n",
    "\n",
    "        eval_iter = DataLoader(CFDataset(torch.tensor(eval_X.astype(np.float32)), torch.LongTensor(lst_temp)),\n",
    "                               batch_size=32, shuffle=False)\n",
    "\n",
    "    else:\n",
    "        train_X = df_train.iloc[:, 1:-1].values.astype(np.float32)\n",
    "        lst_temp = [1 for _ in range(len(train_X))]\n",
    "\n",
    "        train_iter = DataLoader(CFDataset(torch.tensor(train_X.astype(np.float32)), torch.LongTensor(lst_temp)),\n",
    "                                batch_size=128, shuffle=True, worker_init_fn=np.random.seed(0))\n",
    "        # print(len(train_X))\n",
    "        eval_X = df_eval.iloc[:, 1:-1].values.astype(np.float32)\n",
    "        lst_temp = [1 for _ in range(len(eval_X))]\n",
    "\n",
    "        eval_iter = DataLoader(CFDataset(torch.tensor(eval_X.astype(np.float32)), torch.LongTensor(lst_temp)),\n",
    "                               batch_size=32, shuffle=False)\n",
    "    return train_iter, eval_iter, scaler\n",
    "\n",
    "def get_pretrain_result(gaes, aae_trainer, df_test, df_test_cf=[], ratio=1, scaler=None, device='cuda:0'):\n",
    "    R_aae = aae_trainer.max_dist * ratio\n",
    "    test_iter = DataLoader(scaler.transform(df_test.iloc[:, 1:-3].values.astype(np.float32)), batch_size=32, shuffle=False)\n",
    "    lst_pred, lst_score = aae_trainer._evaluation(test_iter, df_test['label'], r=R_aae)\n",
    "    print('Original')\n",
    "    print(classification_report(y_true=df_test['label'], y_pred=lst_pred, digits=5))\n",
    "    print(confusion_matrix(y_true=df_test['label'], y_pred=lst_pred))\n",
    "    print(f\"AUC-PR: {average_precision_score(y_true=df_test['label'], y_score=lst_score)}\")\n",
    "    print(f\"AUC-ROC: {roc_auc_score(y_true=df_test['label'], y_score=lst_score)}\")\n",
    "    df_org = pd.DataFrame()\n",
    "    df_org['label'] = df_test['label'].values\n",
    "    df_org['pred'] = lst_pred\n",
    "\n",
    "    # test_do = gaes.net.get_result(\n",
    "    #     torch.Tensor(df_test.iloc[:, :-3].values.astype(np.float32).reshape(-1, 20, 1)).to(device),\n",
    "    #     do=1).detach().cpu().numpy()\n",
    "    # test_iter = DataLoader(test_do.reshape(-1, 20)[:, 1:], batch_size=32, shuffle=False)\n",
    "    # lst_pred = aae_trainer._evaluation(test_iter, df_test_cf['label'], r=R_aae)\n",
    "    # print('Do')\n",
    "    # print(classification_report(y_true=df_test_cf['label'], y_pred=lst_pred, digits=5))\n",
    "    # print(confusion_matrix(y_true=df_test_cf['label'], y_pred=lst_pred))\n",
    "    # print(f\"AUC-PR: {average_precision_score(y_true=df_test_cf['label'], y_score=lst_pred)}\")\n",
    "    # print(f\"AUC-ROC: {roc_auc_score(y_true=df_test_cf['label'], y_score=lst_pred)}\")\n",
    "    # df_org['pred_do'] = lst_pred\n",
    "\n",
    "\n",
    "    if len(df_test_cf) >= 1:\n",
    "        test_iter = DataLoader(scaler.transform(df_test_cf.iloc[:, 1:-3].values.astype(np.float32).reshape(-1, 19)), batch_size=32,\n",
    "                               shuffle=False)\n",
    "        lst_pred, lst_score  = aae_trainer._evaluation(test_iter, df_test_cf['label'], r=R_aae)\n",
    "        print('CF')\n",
    "        print(classification_report(y_true=df_test_cf['label'], y_pred=lst_pred, digits=5))\n",
    "        print(confusion_matrix(y_true=df_test_cf['label'], y_pred=lst_pred))\n",
    "        print(f\"AUC-PR: {average_precision_score(y_true=df_test_cf['label'], y_score=lst_score)}\")\n",
    "        print(f\"AUC-ROC: {roc_auc_score(y_true=df_test_cf['label'], y_score=lst_score)}\")\n",
    "        df_org['pred_cf'] = lst_pred\n",
    "\n",
    "    return df_org\n",
    "\n",
    "\n",
    "def get_pretrain_results_adult(aae_trainer, df_test, test_do):\n",
    "    R_aae = aae_trainer.max_dist\n",
    "    test_iter = DataLoader(df_test.iloc[:, 1:-1].values.astype(np.float32), batch_size=32,\n",
    "                           shuffle=False)\n",
    "    lst_pred, lst_score = aae_trainer._evaluation(test_iter, df_test['y'], r=R_aae)\n",
    "    print('Original')\n",
    "    print(classification_report(y_true=df_test['y'], y_pred=lst_pred, digits=5))\n",
    "    print(confusion_matrix(y_true=df_test['y'], y_pred=lst_pred))\n",
    "    print(f\"AUC-PR: {average_precision_score(y_true=df_test['y'], y_score=lst_score)}\")\n",
    "    print(f\"AUC-ROC: {roc_auc_score(y_true=df_test['y'], y_score=lst_score)}\")\n",
    "    df_org = pd.DataFrame()\n",
    "    df_org['label'] = df_test['y'].values\n",
    "    df_org['pred'] = lst_pred\n",
    "\n",
    "    test_iter = DataLoader(test_do.astype(np.float32),batch_size=32,shuffle=False)\n",
    "    lst_pred, lst_score = aae_trainer._evaluation(test_iter, df_test['y'], r=R_aae)\n",
    "    print('Generated CF')\n",
    "    print(classification_report(y_true=df_test['y'], y_pred=lst_pred, digits=5))\n",
    "    print(confusion_matrix(y_true=df_test['y'], y_pred=lst_pred))\n",
    "    print(f\"AUC-PR: {average_precision_score(y_true=df_test['y'], y_score=lst_score)}\")\n",
    "    print(f\"AUC-ROC: {roc_auc_score(y_true=df_test['y'], y_score=lst_score)}\")\n",
    "    df_org['pred_do'] = lst_pred\n",
    "\n",
    "    return df_org\n",
    "\n",
    "\n",
    "def retrain_split(gaes, df_train, df_eval, scaler, device='cuda:0', adult=0):\n",
    "    if not adult:\n",
    "        X_do = gaes.net.get_result(torch.Tensor(df_train.iloc[:, :-3].values.astype(np.float32).reshape(-1, 20, 1)).to(device),\n",
    "                                   do=1).detach().cpu().numpy().reshape(-1, 20)[:, 1:]\n",
    "        # X_do = np.delete(X_do, df_train_cf.loc[df_train_cf['label'] == 1].index, axis=0)\n",
    "        train_X_do = X_do\n",
    "        train_X_or = df_train.iloc[:, 1:-3].values.astype(np.float32)\n",
    "        lst_temp = [1 for _ in range(len(train_X_do))]\n",
    "        lst_temp.extend([0 for _ in range(len(train_X_or))])\n",
    "        train_X = scaler.transform(np.concatenate((train_X_do, train_X_or), axis=0))\n",
    "\n",
    "        train_iter = DataLoader(CFDataset(torch.tensor(train_X.astype(np.float32)), torch.LongTensor(lst_temp)),\n",
    "                                batch_size=128, shuffle=True, worker_init_fn=np.random.seed(0))\n",
    "        print(len(train_X))\n",
    "        eval_X_do = gaes.net.get_result(\n",
    "            torch.Tensor(df_eval.iloc[:, :-3].values.astype(np.float32).reshape(-1, 20, 1)).to(device),\n",
    "            do=1).detach().cpu().numpy().reshape(-1, 20)[:, 1:]\n",
    "        eval_X_or = df_eval.iloc[:, 1:-3].values.astype(np.float32)\n",
    "        lst_temp = [1 for _ in range(len(eval_X_do))]\n",
    "        lst_temp.extend([0 for _ in range(len(eval_X_or))])\n",
    "        eval_X = scaler.transform(np.concatenate((eval_X_do, eval_X_or), axis=0))\n",
    "\n",
    "        eval_iter = DataLoader(CFDataset(torch.tensor(eval_X.astype(np.float32)), torch.LongTensor(lst_temp)),\n",
    "                               batch_size=32, shuffle=False)\n",
    "    else:\n",
    "        X_do = gaes.net.get_result(\n",
    "            torch.Tensor(df_train.iloc[:, :-1].values.astype(np.float32).reshape(len(df_train), -1, 1)).to(device),\n",
    "            do=1).detach().cpu().numpy().reshape(len(df_train), -1)[:, 1:]\n",
    "        # X_do = np.delete(X_do, df_train_cf.loc[df_train_cf['label'] == 1].index, axis=0)\n",
    "        train_X_do = X_do\n",
    "        train_X_or = df_train.iloc[:, 1:-1].values.astype(np.float32)\n",
    "        lst_temp = [1 for _ in range(len(train_X_do))]\n",
    "        lst_temp.extend([0 for _ in range(len(train_X_or))])\n",
    "        train_X = np.concatenate((train_X_do, train_X_or), axis=0)\n",
    "\n",
    "        train_iter = DataLoader(CFDataset(torch.tensor(train_X.astype(np.float32)), torch.LongTensor(lst_temp)),\n",
    "                                batch_size=128, shuffle=True, worker_init_fn=np.random.seed(0))\n",
    "        print(len(train_X))\n",
    "        eval_X_do = gaes.net.get_result(\n",
    "            torch.Tensor(df_eval.iloc[:, :-1].values.astype(np.float32).reshape(len(df_eval), -1, 1)).to(device),\n",
    "            do=1).detach().cpu().numpy().reshape(len(df_eval), -1)[:, 1:]\n",
    "        eval_X_or = df_eval.iloc[:, 1:-1].values.astype(np.float32)\n",
    "        lst_temp = [1 for _ in range(len(eval_X_do))]\n",
    "        lst_temp.extend([0 for _ in range(len(eval_X_or))])\n",
    "        eval_X = np.concatenate((eval_X_do, eval_X_or), axis=0)\n",
    "\n",
    "        eval_iter = DataLoader(CFDataset(torch.tensor(eval_X.astype(np.float32)), torch.LongTensor(lst_temp)),\n",
    "                               batch_size=32, shuffle=False)\n",
    "\n",
    "    return train_iter, eval_iter\n",
    "\n",
    "def get_retrain_result(gaes, aae_trainer, df_test, df_test_cf=[], ratio=1, scaler=None):\n",
    "    R_aae = aae_trainer.max_dist * ratio\n",
    "    test_iter = DataLoader(scaler.transform(df_test.iloc[:, 1:-3].values.astype(np.float32)), batch_size=32, shuffle=False)\n",
    "    lst_pred, lst_score = aae_trainer._evaluation(test_iter, df_test['label'], r=R_aae)\n",
    "    print('Original')\n",
    "    print(classification_report(y_true=df_test['label'], y_pred=lst_pred, digits=5))\n",
    "    print(confusion_matrix(y_true=df_test['label'], y_pred=lst_pred))\n",
    "    print(f\"AUC-PR: {average_precision_score(y_true=df_test['label'], y_score=lst_score)}\")\n",
    "    print(f\"AUC-ROC: {roc_auc_score(y_true=df_test['label'], y_score=lst_score)}\")\n",
    "    df_ad = pd.DataFrame()\n",
    "    df_ad['label'] = df_test['y'].values\n",
    "    df_ad['pred'] = lst_pred\n",
    "\n",
    "    # test_do = gaes.net.get_result(\n",
    "    #     torch.Tensor(df_test.iloc[:, :-3].values.astype(np.float32).reshape(-1, 20, 1)).cuda(),\n",
    "    #     do=1).detach().cpu().numpy()\n",
    "    # test_iter = DataLoader(test_do.reshape(-1, 20)[:, 1:], batch_size=32, shuffle=False)\n",
    "    # lst_pred = aae_trainer._evaluation(test_iter, df_test_cf['label'], r=R_aae)\n",
    "    # print('Do')\n",
    "    # print(classification_report(y_true=df_test_cf['label'], y_pred=lst_pred, digits=5))\n",
    "    # print(confusion_matrix(y_true=df_test_cf['label'], y_pred=lst_pred))\n",
    "    # print(f\"AUC-PR: {average_precision_score(y_true=df_test_cf['label'], y_score=lst_pred)}\")\n",
    "    # print(f\"AUC-ROC: {roc_auc_score(y_true=df_test_cf['label'], y_score=lst_pred)}\")\n",
    "    # df_ad['pred_do'] = lst_pred\n",
    "\n",
    "    if len(df_test_cf) >= 1:\n",
    "        test_iter = DataLoader(scaler.transform(df_test_cf.iloc[:, 1:-3].values.astype(np.float32).reshape(-1, 19)), batch_size=32,\n",
    "                               shuffle=False)\n",
    "        lst_pred, lst_score = aae_trainer._evaluation(test_iter, df_test_cf['label'], r=R_aae)\n",
    "        print('CF')\n",
    "        print(classification_report(y_true=df_test_cf['label'], y_pred=lst_pred, digits=5))\n",
    "        print(confusion_matrix(y_true=df_test_cf['label'], y_pred=lst_pred))\n",
    "        print(f\"AUC-PR: {average_precision_score(y_true=df_test_cf['label'], y_score=lst_score)}\")\n",
    "        print(f\"AUC-ROC: {roc_auc_score(y_true=df_test_cf['label'], y_score=lst_score)}\")\n",
    "        df_ad['pred_cf'] = lst_pred\n",
    "\n",
    "    return df_ad\n",
    "\n",
    "\n",
    "def get_retrain_results_adult(aae_trainer, df_test, test_do):\n",
    "    R_aae = aae_trainer.max_dist\n",
    "    test_iter = DataLoader(df_test.iloc[:, 1:-1].values.astype(np.float32), batch_size=32,\n",
    "                           shuffle=False)\n",
    "    lst_pred, lst_score = aae_trainer._evaluation(test_iter, df_test['y'], r=R_aae)\n",
    "    print('Original')\n",
    "    print(classification_report(y_true=df_test['y'], y_pred=lst_pred, digits=5))\n",
    "    print(confusion_matrix(y_true=df_test['y'], y_pred=lst_pred))\n",
    "    print(f\"AUC-PR: {average_precision_score(y_true=df_test['y'], y_score=lst_score)}\")\n",
    "    print(f\"AUC-ROC: {roc_auc_score(y_true=df_test['y'], y_score=lst_score)}\")\n",
    "    df_ad = pd.DataFrame()\n",
    "    df_ad['label'] = df_test['y'].values\n",
    "    df_ad['pred'] = lst_pred\n",
    "\n",
    "    test_iter = DataLoader(test_do.astype(np.float32), batch_size=32, shuffle=False)\n",
    "    lst_pred, lst_score = aae_trainer._evaluation(test_iter, df_test['y'], r=R_aae)\n",
    "    print('Generated CF')\n",
    "    print(classification_report(y_true=df_test['y'], y_pred=lst_pred, digits=5))\n",
    "    print(confusion_matrix(y_true=df_test['y'], y_pred=lst_pred))\n",
    "    print(f\"AUC-PR: {average_precision_score(y_true=df_test['y'], y_score=lst_score)}\")\n",
    "    print(f\"AUC-ROC: {roc_auc_score(y_true=df_test['y'], y_score=lst_score)}\")\n",
    "    df_ad['pred_do'] = lst_pred\n",
    "\n",
    "    return df_ad\n",
    "\n",
    "\n",
    "def get_fairness_result(df_org, df_ad, cf=0):\n",
    "    # df_org['do_changed'] = df_org['pred_do'] - df_org['pred']\n",
    "    # df_ad['do_changed'] = df_ad['pred_do'] - df_ad['pred']\n",
    "    assert len(df_org) == len(df_ad), 'Length should be the same!'\n",
    "    total = len(df_org)\n",
    "    # df_org_do = df_org.groupby(['do_changed']).count().reset_index(drop=False)\n",
    "    # before_do = sum(df_org_do.loc[df_org_do['do_changed'] != 0]['label'].values)\n",
    "    # df_ad_do = df_ad.groupby(['do_changed']).count().reset_index(drop=False)\n",
    "    # after_do = sum(df_ad_do.loc[df_ad_do['do_changed'] != 0]['label'].values)\n",
    "    # print('Results for DO samples')\n",
    "    # print(f'Without fair, the prediction changed: {before_do/total}')\n",
    "    # print(f'With fair, the prediction changed: {after_do/total}')\n",
    "\n",
    "\n",
    "    if cf:\n",
    "        df_org['cf_changed'] = df_org['pred_cf'] - df_org['pred']\n",
    "        df_ad['cf_changed'] = df_ad['pred_cf'] - df_ad['pred']\n",
    "        df_org_cf = df_org.groupby(['cf_changed']).count().reset_index(drop=False)\n",
    "        before_cf = sum(df_org_cf.loc[df_org_cf['cf_changed'] != 0]['label'].values)\n",
    "        df_ad_cf = df_ad.groupby(['cf_changed']).count().reset_index(drop=False)\n",
    "        after_cf = sum(df_ad_cf.loc[df_ad_cf['cf_changed'] != 0]['label'].values)\n",
    "        print('Results for CF samples')\n",
    "        print(f'Without fair, the prediction changed: {before_cf / total}')\n",
    "        print(f'With fair, the prediction changed: {after_cf / total}')\n",
    "    else:\n",
    "        df_org['do_changed'] = df_org['pred_do'] - df_org['pred']\n",
    "        df_ad['do_changed'] = df_ad['pred_do'] - df_ad['pred']\n",
    "        df_org_cf = df_org.groupby(['do_changed']).count().reset_index(drop=False)\n",
    "        before_cf = sum(df_org_cf.loc[df_org_cf['do_changed'] != 0]['label'].values)\n",
    "        df_ad_cf = df_ad.groupby(['do_changed']).count().reset_index(drop=False)\n",
    "        after_cf = sum(df_ad_cf.loc[df_ad_cf['do_changed'] != 0]['label'].values)\n",
    "        print('Results for Generated CF samples')\n",
    "        print(f'Without fair, the prediction changed: {before_cf / total}')\n",
    "        print(f'With fair, the prediction changed: {after_cf / total}')\n",
    "\n",
    "def load_data(adult=0):\n",
    "    if not adult:\n",
    "        df_train = pd.read_csv('../data/train.csv', index_col=0)\n",
    "        df_test = pd.read_csv('../data/test.csv', index_col=0)\n",
    "        df_test_cf = pd.read_csv('../data/test_cf.csv', index_col=0)\n",
    "    else:\n",
    "        try:\n",
    "            df_train = pd.read_csv('../data/adult_train.csv', index_col=0)\n",
    "            df_test = pd.read_csv('../data/adult_test.csv', index_col=0)\n",
    "            df_test_cf = pd.read_csv('../data/adult_do.csv', index_col=0)\n",
    "        except:\n",
    "            df_train = pd.read_csv('data/adult_train.csv', index_col=0)\n",
    "            df_test = pd.read_csv('data/adult_test.csv', index_col=0)\n",
    "            df_test_cf = pd.read_csv('data/adult_do.csv', index_col=0)\n",
    "    return df_train, df_test, df_test_cf\n",
    "\n",
    "def adult_preprocessing(dir='data/adult.data', n_train=10000, n_test=2000):\n",
    "    df_data = pd.read_csv(dir, header=None, names=['age', 'workclass', 'fnlwgt', 'education',\n",
    "                                                                    'education-num', 'marital-status', 'occupation',\n",
    "                                                                    'relationship', 'race', 'sex', 'capital-gain',\n",
    "                                                                    'capital-loss',\n",
    "                                                                    'hours-per-week', 'native-country', 'y'])\n",
    "    for i in range(len(df_data.columns)):\n",
    "        most_frequent = df_data.iloc[:, i].value_counts()[:1].index.tolist()[0]\n",
    "        for j in range(len(df_data.iloc[:, i])):\n",
    "            if df_data.iloc[j, i] == '?':\n",
    "                df_data.iloc[j, i] = most_frequent\n",
    "    df_data.loc[df_data['y'] == ' >50K', 'y'] = 1\n",
    "    df_data.loc[df_data['y'] == ' <=50K', 'y'] = 0\n",
    "    df_data.loc[df_data['sex'] == ' Female', 'sex'] = 1\n",
    "    df_data.loc[df_data['sex'] == ' Male', 'sex'] = -1\n",
    "    for i in ['workclass', 'marital-status', 'occupation', 'education', 'relationship', 'race', 'native-country', 'marital-status']:\n",
    "        data = Counter(df_data[i].values)\n",
    "        val = data.most_common(1)[0][0]\n",
    "        df_data.loc[df_data[i] != val, i] = 0\n",
    "        df_data.loc[df_data[i] == val, i] = 1\n",
    "    df_data = df_data[['sex', 'age', 'native-country', 'race', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation',\n",
    "                       'relationship', 'capital-gain', 'capital-loss',\n",
    "                       'hours-per-week', 'y']]\n",
    "    scaler = MinMaxScaler((-3,3))\n",
    "    df_data = df_data.sample(n=len(df_data), random_state=42)\n",
    "    df_data['y'] = df_data['y'].astype(int)\n",
    "    df_n = df_data.loc[df_data['y'] == 0].copy()\n",
    "    df_n.iloc[:, 1:-1] = scaler.fit_transform(df_n.iloc[:, 1:-1].values)\n",
    "    df_ab = df_data.loc[df_data['y'] == 1].copy()\n",
    "    df_ab.iloc[:, 1:-1] = scaler.transform(df_ab.iloc[:, 1:-1].values)\n",
    "    df_train = df_n.iloc[:n_train]\n",
    "    df_test = pd.concat([df_n.iloc[n_train:n_train+n_train], df_ab.iloc[:n_test]])\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A11clHtAaIYc",
    "outputId": "19136b1a-6e3c-47dd-f2a4-716e8d37d510"
   },
   "outputs": [],
   "source": [
    "df_train, df_test, df_do = load_data(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "CFY1cJd1Z63f"
   },
   "outputs": [],
   "source": [
    "def acc(Y, dist):\n",
    "    \"\"\"\n",
    "    Calculate the AUC, Fgap, and Frank\n",
    "    Args:\n",
    "        dset: dataset\n",
    "        Y: ground truth outlier label\n",
    "        dist: distance to cluster centers\n",
    "    Returns: AUC, Fgap, Frank\n",
    "    \"\"\"\n",
    "\n",
    "    outlier_score, position = torch.min(dist, dim=1)\n",
    "    for i in range(dist.shape[1]):\n",
    "        pos = list(x for x in range(len(outlier_score)) if position[x] == i)\n",
    "        if len(outlier_score[pos]) != 0:\n",
    "            max_dist = max(outlier_score[pos])\n",
    "            outlier_score[pos] = torch.div(outlier_score[pos], max_dist).to(cuda)\n",
    "    if len(set(Y)) > 1:\n",
    "        AUC = roc_auc_score(y_true=Y, y_score=outlier_score.data.cpu().numpy())\n",
    "        PR = average_precision_score(y_true=Y, y_score=outlier_score.data.cpu().numpy())\n",
    "    else:\n",
    "        AUC = -1\n",
    "        PR = -1\n",
    "    return AUC, PR, outlier_score.data.cpu().numpy()\n",
    "\n",
    "\n",
    "def target_distribution(q):\n",
    "    \"\"\"\n",
    "    Calculate the auxiliary distribution with the original distribution\n",
    "    Args:\n",
    "        q: original distribution\n",
    "    Returns: auxiliary distribution\n",
    "    \"\"\"\n",
    "    weight = (q ** 2) / q.sum(0)\n",
    "    return torch.div(weight.t(), weight.sum(1)).t().data\n",
    "\n",
    "\n",
    "def kld(q, p):\n",
    "    \"\"\"\n",
    "    KL-divergence\n",
    "    Args:\n",
    "        q: original distribution\n",
    "        p: auxiliary distribution\n",
    "    Returns: the similarity between two probability distributions\n",
    "    \"\"\"\n",
    "    return torch.sum(p * torch.log(p / q).to(cuda), dim=-1)\n",
    "\n",
    "\n",
    "def getTDistribution(model, x):\n",
    "    \"\"\"\n",
    "    Obtain the distance to centroid for each instance, and calculate the weight module based on that\n",
    "    Args:\n",
    "        model: DCFOD\n",
    "        x: embedded x\n",
    "    Returns: weight module, clustering distribution\n",
    "    \"\"\"\n",
    "\n",
    "    # dist, dist_to_centers = model.module.getDistanceToClusters(x)\n",
    "    dist, dist_to_centers = model.getDistanceToClusters(x)\n",
    "\n",
    "    # -----find the centroid for each instance, with their distance in between----- #\n",
    "    outlier_score, centroid = torch.min(dist_to_centers, dim=1)\n",
    "\n",
    "    # -----for each instance, calculate a score\n",
    "    # by the outlier_score divided by the furtherest instance in the centroid----- #\n",
    "    for i in range(dist_to_centers.shape[1]):\n",
    "        pos = list(x for x in range(len(outlier_score)) if centroid[x] == i)\n",
    "        if len(outlier_score[pos]) != 0:\n",
    "            max_dist = max(outlier_score[pos])\n",
    "            outlier_score[pos] = torch.div(outlier_score[pos], max_dist).to(cuda)\n",
    "    sm = nn.Softmax(dim=0).to(cuda)\n",
    "    weight = sm(outlier_score.neg())\n",
    "\n",
    "    # -----calculate the clustering distribution with the distance----- #\n",
    "    q = 1.0 / (1.0 + (dist / model.alpha))\n",
    "    q = q ** (model.alpha + 1.0) / 2.0\n",
    "    q = (q.t() / torch.sum(q, 1)).t()\n",
    "    return weight, q\n",
    "\n",
    "\n",
    "def clustering(model, mbk, x):\n",
    "    \"\"\"\n",
    "    Initialize cluster centroids with minibatch Kmeans\n",
    "    Args:\n",
    "        model: DCFOD\n",
    "        mbk: minibatch Kmeans\n",
    "        x: embedded x\n",
    "    Returns: N/A\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    x_e = model(x.float())\n",
    "    mbk.partial_fit(x_e.data.cpu().numpy())\n",
    "    model.cluster_centers = mbk.cluster_centers_  # keep the cluster centers\n",
    "    model.clusterCenter.data = torch.from_numpy(model.cluster_centers).to(cuda)\n",
    "\n",
    "\n",
    "def Train(model, train_input, labels, attribute, epochs, batch, with_weight=False, ks=8, kf=100):\n",
    "    \"\"\"\n",
    "    Train DCFOD in minibatch\n",
    "    Args:\n",
    "        model: DCFOD\n",
    "        train_input: input data\n",
    "        labels: ground truth outlier score, which will not be used during training\n",
    "        attribute: sensitive attribute subgroups\n",
    "        epochs: total number of iterations\n",
    "        batch: minibatch size\n",
    "        with_weight: if training with weight\n",
    "        ks: hyperparameter for self-reconstruction loss\n",
    "        kf: hyperparameter for fairness-adversarial loss\n",
    "    Returns: AUC, Fgap, Frank\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    mbk = MiniBatchKMeans(n_clusters=model.num_classes, n_init=20, batch_size=batch)\n",
    "    got_cluster_center = False\n",
    "    running_loss = 0.0\n",
    "    fair_loss = 0.0\n",
    "    lr_cluster = 0.0001\n",
    "    lr_discriminator = 0.00001\n",
    "    lr_sae = 0.00001\n",
    "    optimizer = optim.Adam([\n",
    "        {'params': model.encoder.parameters()},\n",
    "        {'params': model.decoder.parameters()},\n",
    "        {'params': model.discriminator.parameters(), 'lr': lr_discriminator},\n",
    "        {'params': model.clusterCenter, 'lr': lr_cluster}\n",
    "    ], lr=lr_sae, weight_decay=1e-1)\n",
    "#     optimizer = optim.SGD([\n",
    "#         {'params': model.encoder.parameters()},\n",
    "#         {'params': model.decoder.parameters()},\n",
    "#         {'params': model.discriminator.parameters(), 'lr': lr_discriminator},\n",
    "#         {'params': model.clusterCenter, 'lr': lr_cluster}\n",
    "#     ], lr=lr_sae, momentum=0.9)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    print(f'Learning rate: {lr_cluster}, {lr_sae}, {lr_discriminator}')\n",
    "    print(f'batch size: {batch}, self_recon: {ks}, fairness: {kf}')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(train_input.shape[0] // batch):\n",
    "            input_batch = train_input[i * batch: (i + 1) * batch]\n",
    "            x = torch.tensor(input_batch).float()\n",
    "            x = x.to(cuda)\n",
    "\n",
    "            attribute_batch = attribute[i * batch: (i + 1) * batch]\n",
    "            attribute_batch = torch.tensor(attribute_batch).to(cuda).long()\n",
    "            # -----use minibatch Kmeans to initialize the cluster centroids for the clustering layer----- #\n",
    "            if not got_cluster_center:\n",
    "                # model.module.set_clustering_mode(True)\n",
    "                model.setClusteringMode(True)\n",
    "                total = torch.tensor(train_input).to(cuda)\n",
    "                clustering(model, mbk, total)\n",
    "                got_cluster_center = True\n",
    "                # model.module.set_clustering_mode(False)\n",
    "                model.setClusteringMode(False)\n",
    "            else:\n",
    "                model.train()\n",
    "                x_e, x_de, x_sa = model(x)\n",
    "                # -----obtain the clustering probability distribution and dynamic weight----- #\n",
    "                weight, q = getTDistribution(model, x_e)\n",
    "                if x.shape != x_de.shape:\n",
    "                    x = np.reshape(x.data.cpu().numpy(), x_de.shape)\n",
    "                    x = torch.tensor(x).to(cuda)\n",
    "                p = target_distribution(q)\n",
    "                clustering_regularizer_loss = kld(q, p)\n",
    "\n",
    "                self_reconstruction_loss = nn.functional.mse_loss(x_de, x, reduction='none').to(cuda)\n",
    "                self_reconstruction_loss = torch.sum(self_reconstruction_loss, dim=2)\n",
    "                self_reconstruction_loss = torch.reshape(self_reconstruction_loss, (self_reconstruction_loss.shape[0],))\n",
    "\n",
    "                CELoss = nn.CrossEntropyLoss().to(cuda)\n",
    "                discriminator_loss = CELoss(x_sa, attribute_batch)\n",
    "\n",
    "                if with_weight:\n",
    "                    objective = ks * self_reconstruction_loss + kf * discriminator_loss + clustering_regularizer_loss\n",
    "#                     L = objective.mean()\n",
    "                    L = torch.sum(torch.mul(objective, weight))\n",
    "                else:\n",
    "                    objective = ks * self_reconstruction_loss + kf * discriminator_loss + clustering_regularizer_loss\n",
    "                    L = objective.mean()\n",
    "                optimizer.zero_grad()\n",
    "                L.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += L.data.cpu().numpy()\n",
    "                fair_loss += discriminator_loss.data.cpu().numpy()\n",
    "\n",
    "                # -----show loss every 20 mini-batches----- #\n",
    "                if i % 30 == 29:\n",
    "                    print(f'[{epoch + 1},     {i + 1}] L:{running_loss / 30:.2f}, FairLoss: {fair_loss / 30:.4f}')\n",
    "                    running_loss = 0.0\n",
    "                    fair_loss = 0.0\n",
    "\n",
    "        if epoch == epochs-1:\n",
    "            normal_dist = get_abdist(model, train_input, labels)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    print('Done Training.')\n",
    "    return normal_dist\n",
    "\n",
    "def get_abdist(model, train_input, Y, quantile=0.95):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.eval()\n",
    "    model.setValidateMode(True)\n",
    "    model_input = torch.tensor(train_input).to(cuda)\n",
    "    xe = model(model_input.float())\n",
    "    _, dist = model.getDistanceToClusters(xe)\n",
    "    model.setValidateMode(False)\n",
    "    print(acc(Y, dist)[-1])\n",
    "    return np.quantile(dist.detach().cpu().numpy(), quantile)\n",
    "\n",
    "def validate(model, eval_input, Y, normal_dist):\n",
    "    \"\"\"\n",
    "    check the model performance after one iteration of minibatch training\n",
    "    Args:\n",
    "        model: DCFOD\n",
    "        eval_input: input data\n",
    "        Y: ground truth outlier labels\n",
    "    Returns: AUC, Fgap, Frank\n",
    "    \"\"\"\n",
    "\n",
    "    # -----empty cache to save memory for kdd dataset, or have to use DataParellel----- #\n",
    "    torch.cuda.empty_cache()\n",
    "    model.eval()\n",
    "\n",
    "    # -----set model to validate mode, so it only returns the embedded space----- #\n",
    "    # model.module.setTrainValidateMode(True)\n",
    "    model.setValidateMode(True)\n",
    "    model_input = torch.tensor(eval_input).to(cuda)\n",
    "    xe = model(model_input.float())\n",
    "\n",
    "    # -----obtain all instances' distance to cluster centroids----- #\n",
    "    # _, dist = model.module.getDistanceToClusters(x)\n",
    "    _, dist = model.getDistanceToClusters(xe)\n",
    "\n",
    "    # -----set to retrieve AUC, Fgap, Frank values in acc function----- #\n",
    "    AUC, PR, max_dist = acc(Y, dist)\n",
    "    # model.module.setTrainValidateMode(False)\n",
    "    model.setValidateMode(False)\n",
    "\n",
    "    y_pred = [0 if x <= normal_dist else 1 for x in dist.detach().cpu().numpy()]\n",
    "    return AUC, PR, y_pred\n",
    "\n",
    "\n",
    "def shuffle(X, Y, S):\n",
    "    \"\"\"\n",
    "    Shuffle the datasets\n",
    "    Args:\n",
    "        X: input data\n",
    "        Y: outlier labels\n",
    "        S: sensitive attribute subgroups\n",
    "    Returns: shuffled sets\n",
    "    \"\"\"\n",
    "    set_seed(0)\n",
    "    random_index = np.random.permutation(X.shape[0])\n",
    "    return X[random_index], Y[random_index], S[random_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bOCXeqhhcQgk",
    "outputId": "75c22a58-70b4-4830-ff9a-cfc4f58c7c0e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.0001, 1e-05, 1e-05\n",
      "batch size: 128, self_recon: 8, fairness: 100\n",
      "[1,     30] L:750.54, FairLoss: 0.6624\n",
      "[1,     60] L:704.28, FairLoss: 0.6630\n",
      "[1,     90] L:580.08, FairLoss: 0.6659\n",
      "[2,     30] L:448.39, FairLoss: 0.7117\n",
      "[2,     60] L:344.20, FairLoss: 0.6395\n",
      "[2,     90] L:334.59, FairLoss: 0.6498\n",
      "[3,     30] L:350.70, FairLoss: 0.7039\n",
      "[3,     60] L:291.63, FairLoss: 0.6264\n",
      "[3,     90] L:263.42, FairLoss: 0.6128\n",
      "[4,     30] L:249.22, FairLoss: 0.6378\n",
      "[4,     60] L:195.11, FairLoss: 0.5636\n",
      "[4,     90] L:174.99, FairLoss: 0.5596\n",
      "[5,     30] L:170.71, FairLoss: 0.5881\n",
      "[5,     60] L:144.66, FairLoss: 0.5358\n",
      "[5,     90] L:135.39, FairLoss: 0.5373\n",
      "[6,     30] L:137.31, FairLoss: 0.5781\n",
      "[6,     60] L:119.44, FairLoss: 0.5347\n",
      "[6,     90] L:117.34, FairLoss: 0.5402\n",
      "[7,     30] L:125.67, FairLoss: 0.5778\n",
      "[7,     60] L:111.61, FairLoss: 0.5315\n",
      "[7,     90] L:110.33, FairLoss: 0.5323\n",
      "[8,     30] L:119.78, FairLoss: 0.5794\n",
      "[8,     60] L:107.87, FairLoss: 0.5292\n",
      "[8,     90] L:107.22, FairLoss: 0.5352\n",
      "[9,     30] L:115.14, FairLoss: 0.5707\n",
      "[9,     60] L:103.55, FairLoss: 0.5239\n",
      "[9,     90] L:104.30, FairLoss: 0.5288\n",
      "[10,     30] L:110.81, FairLoss: 0.5618\n",
      "[10,     60] L:99.75, FairLoss: 0.5182\n",
      "[10,     90] L:100.65, FairLoss: 0.5211\n",
      "[11,     30] L:107.30, FairLoss: 0.5549\n",
      "[11,     60] L:98.76, FairLoss: 0.5204\n",
      "[11,     90] L:99.92, FairLoss: 0.5257\n",
      "[12,     30] L:107.28, FairLoss: 0.5574\n",
      "[12,     60] L:96.62, FairLoss: 0.5173\n",
      "[12,     90] L:98.31, FairLoss: 0.5234\n",
      "[13,     30] L:109.10, FairLoss: 0.5648\n",
      "[13,     60] L:97.21, FairLoss: 0.5158\n",
      "[13,     90] L:98.66, FairLoss: 0.5233\n",
      "[14,     30] L:108.27, FairLoss: 0.5637\n",
      "[14,     60] L:97.38, FairLoss: 0.5178\n",
      "[14,     90] L:97.99, FairLoss: 0.5188\n",
      "[15,     30] L:107.71, FairLoss: 0.5608\n",
      "[15,     60] L:96.57, FairLoss: 0.5154\n",
      "[15,     90] L:98.14, FairLoss: 0.5233\n",
      "[16,     30] L:108.79, FairLoss: 0.5667\n",
      "[16,     60] L:96.75, FairLoss: 0.5183\n",
      "[16,     90] L:98.11, FairLoss: 0.5215\n",
      "[17,     30] L:106.58, FairLoss: 0.5631\n",
      "[17,     60] L:97.55, FairLoss: 0.5180\n",
      "[17,     90] L:99.02, FairLoss: 0.5255\n",
      "[18,     30] L:106.44, FairLoss: 0.5606\n",
      "[18,     60] L:97.02, FairLoss: 0.5168\n",
      "[18,     90] L:96.47, FairLoss: 0.5236\n",
      "[19,     30] L:106.42, FairLoss: 0.5623\n",
      "[19,     60] L:96.07, FairLoss: 0.5101\n",
      "[19,     90] L:97.35, FairLoss: 0.5186\n",
      "[20,     30] L:105.22, FairLoss: 0.5616\n",
      "[20,     60] L:96.35, FairLoss: 0.5193\n",
      "[20,     90] L:97.38, FairLoss: 0.5257\n",
      "[21,     30] L:105.42, FairLoss: 0.5564\n",
      "[21,     60] L:95.95, FairLoss: 0.5122\n",
      "[21,     90] L:97.22, FairLoss: 0.5186\n",
      "[22,     30] L:104.71, FairLoss: 0.5569\n",
      "[22,     60] L:95.89, FairLoss: 0.5146\n",
      "[22,     90] L:97.56, FairLoss: 0.5306\n",
      "[23,     30] L:105.52, FairLoss: 0.5617\n",
      "[23,     60] L:97.56, FairLoss: 0.5168\n",
      "[23,     90] L:97.19, FairLoss: 0.5264\n",
      "[24,     30] L:104.81, FairLoss: 0.5564\n",
      "[24,     60] L:95.19, FairLoss: 0.5099\n",
      "[24,     90] L:97.06, FairLoss: 0.5260\n",
      "[25,     30] L:104.57, FairLoss: 0.5609\n",
      "[25,     60] L:95.42, FairLoss: 0.5156\n",
      "[25,     90] L:96.64, FairLoss: 0.5245\n",
      "[26,     30] L:105.96, FairLoss: 0.5643\n",
      "[26,     60] L:96.67, FairLoss: 0.5170\n",
      "[26,     90] L:96.28, FairLoss: 0.5179\n",
      "[27,     30] L:105.78, FairLoss: 0.5600\n",
      "[27,     60] L:98.03, FairLoss: 0.5133\n",
      "[27,     90] L:97.10, FairLoss: 0.5213\n",
      "[28,     30] L:105.12, FairLoss: 0.5603\n",
      "[28,     60] L:96.88, FairLoss: 0.5116\n",
      "[28,     90] L:96.14, FairLoss: 0.5212\n",
      "[29,     30] L:103.30, FairLoss: 0.5560\n",
      "[29,     60] L:96.46, FairLoss: 0.5187\n",
      "[29,     90] L:95.69, FairLoss: 0.5208\n",
      "[30,     30] L:104.75, FairLoss: 0.5591\n",
      "[30,     60] L:95.77, FairLoss: 0.5200\n",
      "[30,     90] L:96.83, FairLoss: 0.5222\n",
      "[0.9020909  0.8383397  0.83210295 ... 0.8071158  0.96820146 0.82722646]\n",
      "Done Training.\n"
     ]
    }
   ],
   "source": [
    "ks=8\n",
    "kf=100\n",
    "    \n",
    "    \n",
    "# -----load sensitive subgroups----- #\n",
    "sensitive_attribute_group = df_train.iloc[:,0].values\n",
    "input = np.reshape(sensitive_attribute_group, (-1, 1))\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(input)\n",
    "one_hot = enc.transform(input).toarray()\n",
    "sensitive_attribute_group = np.argmax(one_hot, axis=1)\n",
    "\n",
    "# -----load dataset----- #\n",
    "scaler = StandardScaler()\n",
    "# X_norm = scaler.fit_transform(df_train.iloc[:, 1:-1].values)\n",
    "X_norm = df_train.iloc[:, 1:-1].values\n",
    "Y = df_train['y'].values\n",
    "X_norm, Y, sensitive_attribute_group = shuffle(X_norm, Y, sensitive_attribute_group)\n",
    "\n",
    "num_centroid = 1\n",
    "feature_dimension = X_norm.shape[1]\n",
    "embedded_dimension = 64\n",
    "num_subgroups = len(set(sensitive_attribute_group))\n",
    "# configuration = 90, 64 if X_norm.shape[0] < 10000 else 40, 256\n",
    "configuration = 30, 128\n",
    "\n",
    "model = DCFOD(feature_dimension, num_centroid, embedded_dimension, num_subgroups, cuda)\n",
    "normal_dist = Train(model, X_norm, Y, sensitive_attribute_group, configuration[0], configuration[1], with_weight=False, ks=ks, kf=kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start fairness training\n",
      "Learning rate: 0.0001, 1e-05, 1e-05\n",
      "batch size: 128, self_recon: 8, fairness: 100\n",
      "[1,     30] L:756.47, FairLoss: 0.6676\n",
      "[1,     60] L:731.24, FairLoss: 0.6697\n",
      "[1,     90] L:634.75, FairLoss: 0.6704\n",
      "[2,     30] L:499.88, FairLoss: 0.7168\n",
      "[2,     60] L:355.10, FairLoss: 0.6428\n",
      "[2,     90] L:343.78, FairLoss: 0.6596\n",
      "[3,     30] L:360.31, FairLoss: 0.7259\n",
      "[3,     60] L:298.63, FairLoss: 0.6556\n",
      "[3,     90] L:268.65, FairLoss: 0.6525\n",
      "[4,     30] L:259.44, FairLoss: 0.6917\n",
      "[4,     60] L:204.47, FairLoss: 0.6052\n",
      "[4,     90] L:179.64, FairLoss: 0.5817\n",
      "[5,     30] L:168.61, FairLoss: 0.5951\n",
      "[5,     60] L:139.13, FairLoss: 0.5410\n",
      "[5,     90] L:133.10, FairLoss: 0.5425\n",
      "[6,     30] L:138.20, FairLoss: 0.5833\n",
      "[6,     60] L:118.60, FairLoss: 0.5323\n",
      "[6,     90] L:119.11, FairLoss: 0.5416\n",
      "[7,     30] L:125.63, FairLoss: 0.5791\n",
      "[7,     60] L:110.44, FairLoss: 0.5299\n",
      "[7,     90] L:113.29, FairLoss: 0.5434\n",
      "[8,     30] L:121.37, FairLoss: 0.5824\n",
      "[8,     60] L:109.82, FairLoss: 0.5359\n",
      "[8,     90] L:110.49, FairLoss: 0.5482\n",
      "[9,     30] L:116.27, FairLoss: 0.5770\n",
      "[9,     60] L:107.75, FairLoss: 0.5362\n",
      "[9,     90] L:107.29, FairLoss: 0.5418\n",
      "[10,     30] L:114.14, FairLoss: 0.5750\n",
      "[10,     60] L:102.14, FairLoss: 0.5266\n",
      "[10,     90] L:103.10, FairLoss: 0.5344\n",
      "[11,     30] L:110.93, FairLoss: 0.5670\n",
      "[11,     60] L:100.44, FairLoss: 0.5237\n",
      "[11,     90] L:104.02, FairLoss: 0.5375\n",
      "[12,     30] L:109.27, FairLoss: 0.5708\n",
      "[12,     60] L:101.41, FairLoss: 0.5310\n",
      "[12,     90] L:101.77, FairLoss: 0.5396\n",
      "[13,     30] L:109.64, FairLoss: 0.5693\n",
      "[13,     60] L:98.73, FairLoss: 0.5236\n",
      "[13,     90] L:99.64, FairLoss: 0.5285\n",
      "[14,     30] L:109.72, FairLoss: 0.5664\n",
      "[14,     60] L:101.52, FairLoss: 0.5218\n",
      "[14,     90] L:100.53, FairLoss: 0.5318\n",
      "[15,     30] L:110.65, FairLoss: 0.5644\n",
      "[15,     60] L:98.41, FairLoss: 0.5268\n",
      "[15,     90] L:100.16, FairLoss: 0.5313\n",
      "[16,     30] L:109.38, FairLoss: 0.5669\n",
      "[16,     60] L:98.06, FairLoss: 0.5244\n",
      "[16,     90] L:99.71, FairLoss: 0.5327\n",
      "[17,     30] L:108.89, FairLoss: 0.5692\n",
      "[17,     60] L:100.27, FairLoss: 0.5284\n",
      "[17,     90] L:100.31, FairLoss: 0.5312\n",
      "[18,     30] L:107.78, FairLoss: 0.5667\n",
      "[18,     60] L:97.67, FairLoss: 0.5254\n",
      "[18,     90] L:101.53, FairLoss: 0.5335\n",
      "[19,     30] L:107.94, FairLoss: 0.5605\n",
      "[19,     60] L:99.12, FairLoss: 0.5238\n",
      "[19,     90] L:99.88, FairLoss: 0.5338\n",
      "[20,     30] L:107.33, FairLoss: 0.5660\n",
      "[20,     60] L:97.07, FairLoss: 0.5216\n",
      "[20,     90] L:98.47, FairLoss: 0.5279\n",
      "[21,     30] L:107.57, FairLoss: 0.5637\n",
      "[21,     60] L:97.01, FairLoss: 0.5217\n",
      "[21,     90] L:99.19, FairLoss: 0.5290\n",
      "[22,     30] L:106.79, FairLoss: 0.5629\n",
      "[22,     60] L:99.06, FairLoss: 0.5277\n",
      "[22,     90] L:99.80, FairLoss: 0.5301\n",
      "[23,     30] L:107.72, FairLoss: 0.5665\n",
      "[23,     60] L:97.70, FairLoss: 0.5245\n",
      "[23,     90] L:99.82, FairLoss: 0.5345\n",
      "[24,     30] L:106.22, FairLoss: 0.5633\n",
      "[24,     60] L:98.13, FairLoss: 0.5203\n",
      "[24,     90] L:98.48, FairLoss: 0.5202\n",
      "[25,     30] L:106.73, FairLoss: 0.5668\n",
      "[25,     60] L:97.46, FairLoss: 0.5231\n",
      "[25,     90] L:98.86, FairLoss: 0.5299\n",
      "[26,     30] L:105.69, FairLoss: 0.5623\n",
      "[26,     60] L:98.41, FairLoss: 0.5239\n",
      "[26,     90] L:98.12, FairLoss: 0.5276\n",
      "[27,     30] L:108.10, FairLoss: 0.5620\n",
      "[27,     60] L:98.64, FairLoss: 0.5266\n",
      "[27,     90] L:98.86, FairLoss: 0.5316\n",
      "[28,     30] L:107.61, FairLoss: 0.5676\n",
      "[28,     60] L:98.24, FairLoss: 0.5240\n",
      "[28,     90] L:98.67, FairLoss: 0.5257\n",
      "[29,     30] L:107.62, FairLoss: 0.5677\n",
      "[29,     60] L:97.21, FairLoss: 0.5213\n",
      "[29,     90] L:98.46, FairLoss: 0.5264\n",
      "[30,     30] L:106.17, FairLoss: 0.5641\n",
      "[30,     60] L:97.43, FairLoss: 0.5225\n",
      "[30,     90] L:98.11, FairLoss: 0.5244\n",
      "[0.5250669  0.44394836 0.6403511  ... 0.4273668  0.5868171  0.6476393 ]\n",
      "Done Training.\n"
     ]
    }
   ],
   "source": [
    "print('Start fairness training')\n",
    "model_f = DCFOD(feature_dimension, num_centroid, embedded_dimension, num_subgroups, cuda)\n",
    "normal_dist = Train(model_f, X_norm, Y, sensitive_attribute_group, configuration[0], configuration[1],\n",
    "                    with_weight=True, ks=ks, kf=kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9020909  0.8383397  0.83210295 ... 0.8071158  0.96820146 0.82722646]\n",
      "21.08180990219116\n",
      "Before fairness training results:\n",
      "AUC value: 0.5324789583333334\n",
      "PR: 0.187236973088047\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.83101   0.94867   0.88595     12000\n",
      "           1    0.12126   0.03542   0.05482      2400\n",
      "\n",
      "    accuracy                        0.79646     14400\n",
      "   macro avg    0.47613   0.49204   0.47038     14400\n",
      "weighted avg    0.71272   0.79646   0.74743     14400\n",
      "\n",
      "[[11384   616]\n",
      " [ 2315    85]]\n"
     ]
    }
   ],
   "source": [
    "quantile = 0.95\n",
    "R = get_abdist(model, X_norm, Y, quantile)\n",
    "print(R)\n",
    "# test_X = scaler.transform(df_test.iloc[:, 1:-1].values.astype(np.float32))\n",
    "test_X = df_test.iloc[:, 1:-1].values.astype(np.float32)\n",
    "test_Y = df_test['y'].values\n",
    "AUC, PR , pred = validate(model, test_X, test_Y, R)\n",
    "print('Before fairness training results:')\n",
    "print(f'AUC value: {AUC}')\n",
    "print(f'PR: {PR}')\n",
    "print(classification_report(test_Y, pred, digits=5))\n",
    "print(confusion_matrix(test_Y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "wPqujWHragSD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before fairness training results:\n",
      "AUC value: 0.5890895486111111\n",
      "PR: 0.19238729628486403\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.84372   0.79225   0.81717     12000\n",
      "           1    0.20402   0.26625   0.23102      2400\n",
      "\n",
      "    accuracy                        0.70458     14400\n",
      "   macro avg    0.52387   0.52925   0.52410     14400\n",
      "weighted avg    0.73710   0.70458   0.71948     14400\n",
      "\n",
      "[[9507 2493]\n",
      " [1761  639]]\n"
     ]
    }
   ],
   "source": [
    "_, _, unf_pred = validate(model, test_X, test_Y, normal_dist)\n",
    "test_X_cf = df_do.values.astype(np.float32)\n",
    "# test_X_cf = scaler.transform(df_do.values.astype(np.float32))\n",
    "test_Y_cf = df_test['y'].values\n",
    "unf_AUC, unf_PR, unf_pred_cf = validate(model, test_X_cf, test_Y_cf, R)\n",
    "print('Before fairness training results:')\n",
    "print(f'AUC value: {unf_AUC}')\n",
    "print(f'PR: {unf_PR}')\n",
    "print(classification_report(test_Y_cf, unf_pred_cf, digits=5))\n",
    "print(confusion_matrix(test_Y_cf, unf_pred_cf))\n",
    "df_org = pd.DataFrame()\n",
    "df_org['label'] = test_Y\n",
    "df_org['pred'] = pred\n",
    "df_org['pred_cf'] = unf_pred_cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "zBD6MVyLaijS",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5250669  0.44394836 0.6403511  ... 0.4273668  0.5868171  0.6476393 ]\n",
      "After fairness training results :\n",
      "AUC value: 0.5851274826388888\n",
      "PR: 0.2289132067151607\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.84400   0.94725   0.89265     12000\n",
      "           1    0.32082   0.12458   0.17947      2400\n",
      "\n",
      "    accuracy                        0.81014     14400\n",
      "   macro avg    0.58241   0.53592   0.53606     14400\n",
      "weighted avg    0.75680   0.81014   0.77379     14400\n",
      "\n",
      "[[11367   633]\n",
      " [ 2101   299]]\n",
      "After fairness training results CF:\n",
      "AUC value: 0.6140686111111111\n",
      "PR: 0.23570443237191765\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.84432   0.91883   0.88000     12000\n",
      "           1    0.27368   0.15292   0.19620      2400\n",
      "\n",
      "    accuracy                        0.79118     14400\n",
      "   macro avg    0.55900   0.53587   0.53810     14400\n",
      "weighted avg    0.74921   0.79118   0.76604     14400\n",
      "\n",
      "[[11026   974]\n",
      " [ 2033   367]]\n",
      "Results for CF samples\n",
      "Without fair, the prediction changed: 0.2078472222222222\n",
      "With fair, the prediction changed: 0.06090277777777778\n"
     ]
    }
   ],
   "source": [
    "R = get_abdist(model_f, X_norm, Y, quantile)\n",
    "AUC, PR, f_pred = validate(model_f, test_X, test_Y, R)\n",
    "print('After fairness training results :')\n",
    "print(f'AUC value: {AUC}')\n",
    "print(f'PR: {PR}')\n",
    "print(classification_report(test_Y, f_pred, digits=5))\n",
    "print(confusion_matrix(test_Y, f_pred))\n",
    "f_AUC, f_PR, f_pred_cf = validate(model_f, test_X_cf, test_Y_cf, R)\n",
    "print('After fairness training results CF:')\n",
    "print(f'AUC value: {f_AUC}')\n",
    "print(f'PR: {f_PR}')\n",
    "print(classification_report(test_Y_cf, f_pred_cf, digits=5))\n",
    "print(confusion_matrix(test_Y_cf, f_pred_cf))\n",
    "df_ad = pd.DataFrame()\n",
    "df_ad['label'] = test_Y\n",
    "df_ad['pred'] = f_pred\n",
    "df_ad['pred_cf'] = f_pred_cf\n",
    "total = len(df_ad)\n",
    "df_org['cf_changed'] = df_org['pred_cf'] - df_org['pred']\n",
    "df_ad['cf_changed'] = df_ad['pred_cf'] - df_ad['pred']\n",
    "df_org_cf = df_org.groupby(['cf_changed']).count().reset_index(drop=False)\n",
    "before_cf = sum(df_org_cf.loc[df_org_cf['cf_changed'] != 0]['label'].values)\n",
    "df_ad_cf = df_ad.groupby(['cf_changed']).count().reset_index(drop=False)\n",
    "after_cf = sum(df_ad_cf.loc[df_ad_cf['cf_changed'] != 0]['label'].values)\n",
    "print('Results for CF samples')\n",
    "print(f'Without fair, the prediction changed: {before_cf / total}')\n",
    "print(f'With fair, the prediction changed: {after_cf / total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "DCFOD.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
